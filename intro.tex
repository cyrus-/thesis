% !TEX root = omar-thesis.tex
\chapter{Introduction}\label{chap:intro}
%\vspace{-5px}
% \begin{quote}\textit{The recent development of programming languages suggests that the simul\-taneous achievement of simplicity 
% and generality in language design is a serious unsolved 
% problem.}\begin{flushright}--- John Reynolds (1970) \cite{Reynolds70}\end{flushright}
% \end{quote}
%\begin{quote}
%\textit{Try to imagine that you are a tree. How do you want to look out here?}
%\textit{You want your tree to have some character.}
%\begin{flushright} --- Bob Ross, \emph{The Joy of Painting}\end{flushright}
%\end{quote}

%\vspace{-5px}
\section{Motivation}\label{sec:intro-motivation}

%Programming languages come in many sizes. The smallest languages -- for example, the various ``lambda calculi'' -- isolate language primitives of interest for the benefit of students, researchers and language designers interested in studying their mathematical properties. These studies inform the design of ``full-scale'' programming 
%\footnote{Throughout this work, words and phrases that should be read as having an intuitive or informal meaning, rather than a strict mathematical meaning, will be introduced with quotation marks.} 
% languages, which combine several such primitives, or generalizations thereof. Full-scale languages are interesting objects of formal study in their own right. They also serve as useful tools for software developers, allowing them to construct, reason about and modularly organize large software systems.

There are many ways to draw a tree. For example, consider these three drawings:

\begin{subequations}
\begin{equation}\label{simple-example-op-form}\adiv{\anumintro{1}}{
	\apow{\anumintro{2}}{\anumintro{3}}
}\end{equation}
\begin{equation}\label{simple-example-sty-form}
\frac{\numintro{1}}{{\numintro{2}^{\numintro{3}}}}
\end{equation}
\begin{equation}\label{simple-example-txt-form}
\texttt{1 / 2\textasciicircum3}
\end{equation}
\end{subequations}
\noindent
According to the syntax chart in Figure \ref{fig:simple-example}, these are drawings of an \emph{abstract syntax tree (AST)} of sort $\mathsf{Exp}$,\footnote{Some exposure to abstract syntax trees and inference rules is preliminary to this work. See Sec. \ref{sec:preliminaries} for citations and a more thorough discussion of necessary preliminaries.} differing only in that Drawing (\ref*{simple-example-op-form}) is in \emph{operational form}, Drawing (\ref*{simple-example-sty-form}) is in \emph{stylized form} and Drawing (\ref*{simple-example-txt-form}) is in \emph{textual form}. 
% In particular, let us consider a simple programming language, $\simplelang$, for performing arithmetic calculations with numbers. The expressions of $\simplelang$ are \emph{abstract syntax trees (ASTs)} of a sort defined by the syntax chart in Figure \ref{fig:simple-example}.\footnote{Familiarity with abstract syntax trees is preliminary to this work (see Sec. \ref{sec:preliminaries} for other preliminaries.)}  For example, the following expression is drawn in stylized form:
% The same expression is drawn in textual form as follows:
% \noindent
% and in operational form as follows:

ASTs of this sort are the expressions of an arithmetic programming language, $\simplelang$. The semantics of $\simplelang$ identifies expressions {structurally}, i.e. only the rows of the syntax chart in Figure \ref{fig:simple-example} are semantically relevant. For example, consider the  judgement $\isvalU{e}$, which establishes certain expressions as \emph{values} (as distinct from expressions that can be arithmetically simplified.) This judgement is defined by a single inference rule, which establishes that every number expressions is a value:
\begin{equation}\label{rule:num-val}
\inferrule{ }{
	\isvalU{\anumintro{n}}
}
\end{equation}
Although Rule (\ref*{rule:num-val}) is drawn using the operational form for number expressions, we can apply it to derive that $\isvalU{\numintro{2}}$, because $\numintro{2}$ and $\anumintro{2}$ identify the same tree. In short, ``syntax doesn't matter'' from the perspective of the semantics of $\simplelang$.


\begin{figure}
\hspace{-5px}$\begin{array}{lrlllll}
\textbf{Sort} & & & \textbf{Operational Form} & \textbf{Stylized Form} & \textbf{Textual Form} & \textbf{Description}\\
\mathsf{Exp} & e & ::= & \anumintro{n} & \numintro{n} & \numintro{n} & \text{numbers}\\
&&& \aplus{e}{e} & e + e & e\texttt{ + }e & \text{addition} \\
&&& \aminus{e}{e} & e - e & e\texttt{ - }e & \text{subtraction}\\
&&& \amult{e}{e} & e \times e & e\texttt{ * }e & \text{multiplication}\\
&&& \adiv{e}{e} & \frac{e}{e} & e\texttt{ / }e & \text{division}\\
&&& \apow{e}{e} & {e}^{e} & e\verb|^|e & \text{exponentiation}
\end{array}$
\caption[Syntax of $\simplelang$]{Syntax of $\simplelang$. Metavariable $n$ ranges over mathematical numbers (defined in some suitable manner) and $\numintro{n}$ abbreviates the numeral forms (one for each number $n$, drawn in our examples in \texttt{typewriter} font.) A complete definition of the stylized and textual syntax of $\simplelang$ would require 1) defining these numeral forms explicitly; 2) defining a form for parenthesized expressions; 3) defining the precedence and associativity of each infix operator; and 4) defining whitespace conventions. These details are not relevant to the present discussion, so we tacitly assume the usual conventions.}
\label{fig:simple-example}
\end{figure}

From the perspective of a human programmer, of course, syntax \emph{does} matter, i.e. different drawings of a tree are far from indistinguishable. For example, a human might distinguish drawings in stylized form as more ``readable'' than those in operational or textual form, because the stylized forms follow  widely taught arithmetic conventions. Drawings in textual or operational form are distinguished in that they can be drawn using a simple text editor. And drawings in operational form dispense with various syntactic complexities (e.g. related to operator precedence and associativity), which simplifies metatheoretic reasoning and implementation efforts. %Mistakes may also be less frequent when producing drawings in stylized or textual form (for $\simplelang$ expressions, perhaps only because operational forms use more parentheses). 
In short, different drawings of a tree, though semantically indistinguishable, can and must be distinguished by the \emph{cognitive costs}  that human programmers incur as they produce or examine them (we will consider various operationalizations of this necessarily broad notion in Sec. \ref{sec:syntactic-properties}.) %Regardless, it is  apparent that although syntax doesn't matter (semantically), different drawings of .  %that the popular refrain amongst language researchers that syntax ``doesn't matter'' is of rather limited.   %that intuitively,  drawings in stylized or textual form are of lower syntactic cost than those in operational form. This is important not only because aesthetics, but also because syntactic cost relates to measures of programmer productivity and other quantities and qualities of extrinsic interest (we discuss these and other measures in more detail in Sec. \ref{sec:syntactic-properties}). 

%The forms defined by the syntax chart in Figure \ref{fig:simple-example} suffice to allow programmers to draw any $\simplelang$ expression. However, 
In seeking to reduce cognitive cost, syntax designers  often  include additional \emph{derived forms}  in a syntax definition. Unlike the \emph{primitive forms}  defined in Figure \ref{fig:simple-example}, which identify trees immediately, a derived form identifies a tree via a context-independent \emph{desugaring transformation}. 
%We can define a desugaring transformation by stating a rewrite rule. 
For example, the following rewrite rule defines a derived stylized form for calculating the square root of a $\simplelang$ expression:
% \begin{subequations}
\begin{equation}\label{rule:simplelang-sqrt}
\sqrt{e} \rightarrowtriangle e^{\frac{\numintro{1}}{\numintro{2}}}
\end{equation}
% Similarly, the following rewrite rule, if included in the definition of the textual syntax of expressions, defines a derived form for negating a $\simplelang$ expression:\footnote{Notice that the right-hand side of this rule is in operational form, rather than textual. For $\footnotesimplelang$, it is not necessary to prevent textual and operational forms from being interspersed within a single drawing -- no ambiguities can arise. For richer syntax definitions, this may no longer be the case. The desugaring process must then be modified to first convert the pattern on the righthand side of a desugaring rule like Rule (\ref{rule:simplelang-negate}) to the desired form variant before it is applied.}
% \begin{equation}\label{rule:simplelang-negate}
% \texttt{-}e \rightarrowtriangle \amult{e}{\anumintro{-1}}
% \end{equation}
% \end{subequations}
% \noindent 
Desugaring a drawing of a tree involves first recursively desugaring the drawings of its subtrees. If the drawing is in primitive form, desugaring is complete.  If the drawing is in derived form, we apply the corresponding desugaring transformation (here, we  have only one choice, stated as a rewrite rule.) The desugared drawing will identify a tree immediately, i.e. it will consist only of primitive forms. As such, the semantics need not be modified to give meaning to derived forms.
%Similarly, we might define a derived form for taking an arbitrary root of an expression as follows:
% \begin{align*}
% \sqrt[e']{e} & \rightarrowtriangle e^{\frac{\numintro{1}}{e'}}
% \end{align*}

$\simplelang$ is semantically limited -- we can express only simple computations of a single type -- so we should not expect to need many more derived forms to satisfyingly capture the  idioms that would arise in common human usage of the language (a few standard arithmetic notations should suffice.) %Consequently, there is little opportunity to go beyond simple derived forms like these. 
Modern programming languages, in contrast, have substantially richer semantic structure, and they have found application across a wide variety of human problem domains, so it is natural that many more idioms -- and with them, many more derived forms -- have emerged over time.\footnote{The same dynamic is apparent in the progression of ``pen-and-paper'' mathematics.}  
For instance, the textual syntax of Standard ML (SML), a ``general-purpose''  language in the functional tradition, defines derived forms for constructing and pattern matching on lists \cite{mthm97-for-dart,harper1997programming}. In SML, the derived expression (also, pattern) form \lstinline{[1, 2, 3, 4, 5]} desugars to an expression (pattern) equivalent to:
\begin{lstlisting}[numbers=none]
Cons(1, Cons(2, Cons(3, Cons(4, Cons(5, Nil)))))
\end{lstlisting}
assuming \li{Nil} and \li{Cons} stand for the list constructors exported by the SML Basis library (i.e. SML's ``standard library''.)\footnote{The desugaring actually uses unforgeable identifiers bound permanently to the list constructors, to ensure that the desugaring is context independent. We will return to the concept of context independence throughout this work.} Other languages similarly privilege select standard library constructs with derived forms, for example:

\begin{itemize}
\item OCaml defines derived forms for mutable arrays.
\item Haskell defines derived forms for values of monadic type.
\item Scala defines derived XML forms.
\item F\#, Scala and various other languages define derived forms for encodings of the language's own syntax trees (these are often referred to as \emph{quasiquotation} forms.)
\item Python defines derived forms for mutable sets and dictionaries.
\item Perl defines derived regular expression forms.
\end{itemize}

This is, fundamentally, an \emph{ad hoc} design practice -- there are no clear semantic criteria that fundamentally distinguish standard library constructs from  those defined in third-party libraries. Indeed, the OCaml community has recently moved to de-emphasize the standard library in favor of competing bundles of third-party libraries (in particular, Batteries is an open source effort, and Core is a commercially maintained effort.) This suggests that the most parsimonious approach would be to remove the derived forms specific to standard library constructs from language definitions in favor of mechanisms that give more control over form to third-party library providers. 
We will detail various such mechanisms in Section \ref{sec:existing-approaches}. 
%For a language definition to make no mention of a standard library at all, leaving such efforts to one or more separate community processes.

\section{Syntax Dialects}
The most syntactically expressive of the mechanisms that we will detail in Section \ref{sec:existing-approaches} are syntax definition systems like Camlp4 \cite{ocaml-manual}, Copper \cite{conf/gpce/WykS07} and SugarJ/Sugar* \cite{erdweg2011sugarj,erdweg2013framework}. These enable the construction of  
%library-specific (a.k.a. ``domain-specific'') 
\emph{syntax dialects}, i.e. new syntax definitions that extend the original syntax definition with new derived forms. For example, Ur/Web extends Ur's textual syntax with derived forms for SQL queries, XHTML elements and other constructs defined by a  web programming library \cite{conf/popl/Chlipala15}. The program fragment drawn in Figure \ref{fig:urweb} demonstrates how XHTML expressions that contain strings and other XHTML expressions can be drawn in Ur/Web. % Such dialects are sometimes qualitatively taxonomized as amongst the ``domain-specific language'' for this reason \cite{fowler2010domain}. %Syntactic cost is often assessed qualitatively \cite{green1996usability}, though quantitative metrics can be defined. 
\begin{figure}
\begin{lstlisting}[numbers=none]
val p = SURL<xml><p>Hello, {EURLcase name of
                          None => SURL<xml>World</xml>EURL
                        | Some s => SURL<xml>{[EURLsSURL]}</xml>}!</p></xml>EURL
\end{lstlisting}
\caption{Derived XHTML forms in Ur/Web}
\label{fig:urweb}
\end{figure}                           
%The desugaring, not shown, is substantially more verbose and, for programmers who are familiar with XHTML forms, substantially more obscure than the drawing above. %We will consider other examples of data structures where syntactic cost becomes a legitimate concern for client programmers in Sec. \ref{sec:motivating-examples}. 
%after first reviewing simpler approaches that also help library providers control syntactic cost, albeit to a more limited extent, 
%Syntax definition systems  have simplified the task of defining library-specific (a.k.a ``domain-specific'') syntax dialects like Ur/Web, and thereby contributed to their ongoing proliferation.


%Full-scale languages are also interesting objects of mathematical study. Uniquely, however, they are also designed for use by humans. Consequently, their designers  typically define both an abstract syntax and a textual syntax. This textual syntax serves as the primary interface between human programmers and the language, so it is common to define various \emph{derived forms}, i.e. forms defined by a context-independent \emph{desugaring} to a set of \emph{base forms}. These serve to decrease the \emph{syntactic cost} or \emph{cognitive cost} of selected idioms. 
%In some cases, a derived form is designed to capture an idiom77Gu that involves only the primitive constructs of the language. 

%The hope amongst some language designers is that a limited number of derived forms like these will suffice to produce a ``general-purpose'' textual syntax, i.e. one that is accepted as suitable for use across a wide variety of application domains. Alas, a stable design that fully achieves this ideal has yet to emerge, as evidenced by the diverse array of \emph{syntax dialects} -- dialects that introduce only new derived forms -- that continue to proliferate around all major contemporary languages. 

%In fact, tools that aid in the construction of so-called  ``domain-specific'' language dialects (DSLs)\footnote{In some parts of the literature, such dialects are called ``external DSLs'', to distinguish them from  ``internal'' or ``embedded DSLs'', which are actually  library interfaces that only ``resemble'' distinct dialects \cite{fowler2010domain}.} seem only to be becoming more prominent over time. 

%\subsection{Why are there so many language dialects?}
%{This calls for an investigation}: why is it that programmers and researchers are still so often unable to satisfyingly express the constructs that they seek in libraries, as modes of use of the ``general-purpose'' primitives already available in major languages today, and instead see a need for new language dialects?

%Perhaps the most common sort of dialect is the \emph{syntax dialect} -- a dialect that introduces only new derived syntactic forms, motivated by a desire to decrease the {syntactic cost} of working with one or more library constructs of interest. 
%Put another way, syntax dialects can be specified by a context-independent expansion to the existing language that they are based on. 
%For example, Ur/Web is a syntax dialect of Ur (a language that itself descends from ML \cite{conf/pldi/Chlipala10}) that builds in derived forms for SQL queries, HTML elements and other datatypes used in the domain of web programming \cite{conf/popl/Chlipala15}. %Syntactic cost is often assessed qualitatively \cite{green1996usability}, though quantitative metrics can be defined. 
%This is not an isolated example -- we will consider a number of additional types of data that similarly stand to benefit from the availability of specialized derived forms in Sec. \ref{sec:motivating-examples}. 
%Tools like Camlp4 \cite{ocaml-manual}, Sugar* \cite{erdweg2011sugarj,erdweg2013framework} and Racket \cite{Flatt:2012:CLR:2063176.2063195}, which we will discuss in Sec. \ref{sec:existing-approaches}, have lowered the engineering costs of constructing syntax dialects in such situations, further contributing to their proliferation. 

%More advanced dialects introduce new type structure, going beyond what is possible with only new derived forms. As a simple example, the static and dynamic semantics of records cannot be expressed by context-independent expansion to a language with only nullary and binary products. Various languages have explored ``record-like'' primitives that go further, supporting functional update operators, width and depth coercions (sometimes implicit)%\cite{Cardelli:1984:SMI:1096.1098}
%, methods, prototypic dispatch and other such ``semantic embellishments'' that in turn cannot be expressed by context-independent expansion to a language with only standard record types (we will detail an  example in Sec. \ref{sec:metamodules-motivating-examples}). OCaml primitively builds in the type structure of polymorphic variants, open datatypes and  operations that use format strings like $\mathtt{sprintf}$ \cite{ocaml-manual}. ReactiveML builds in primitives for functional reactive programming \cite{mandel2005reactiveml}. ML5 builds in high-level primitives for distributed programming based on a modal lambda calculus \cite{Murphy:2007:TDP:1793574.1793585}. Manticore \cite{conf/popl/FluetRRSX07} and AliceML  \cite{AliceLookingGlass} build in parallel programming primitives with a more elaborate type structure than is found in simpler accounts of parallelism. 
%MLj builds in the type structure of the Java object system (motivated by a desire to interface safely and naturally with Java libraries) \cite{Benton:1999:IWW:317636.317791}. Other dialects do the same for other foreign languages, e.g. Furr and Foster describe a dialect of OCaml that builds in the type structure of C \cite{Furr:2005:CTS:1065010.1065019}. Tools like proof assistants and logical frameworks are used to specify and reason metatheoretically about dialects like these, and tools like compiler generators and language frameworks \cite{erdweg2013state} lower their implementation cost, again contributing to their proliferation. 

% \vspace{-5px}
\subsection{Problems with the Dialect-Oriented Approach}\label{sec:problems-with-dialects}
Some have argued that a proliferation of syntax dialects is harmless or even desirable, because programmers can simply choose the right dialect for each job at hand \cite{journals/stp/Ward94}. However, this ``dialect-oriented approach'' is difficult to reconcile with the best practices of ``programming in the large''  \cite{DeRemer76}, i.e. developing large programs ``consisting of many small programs (modules), possibly written by different people'' whose interactions are mediated by a type and binding discipline. We will introduce the problems that tend to come up here; a more systematic treatment will follow in  Sec. \ref{sec:syntax-dialects}.

\subsubsection{Conservatively Combining Syntax Dialects}
The first problem with the dialect-oriented approach is that programmers cannot always combine different syntax dialects when they want to use the derived forms that they define together within a single module. In other words, problem domains commonly (and increasingly) overlap with one another.

For example, consider a syntax dialect, $\mathcal{H}$, defining derived forms for working with encodings of HTML elements, and another syntax dialect, $\mathcal{R}$,  defining derived forms for working with encodings of regular expressions. In problem domains where both HTML elements and regular expressions are common (e.g. bioinformatics), it would be useful to construct a ``combined dialect'' where all of these derived forms are defined. 

For this notion of ``dialect combination'' to be well-defined at all, we must first have that $\mathcal{H}$ and $\mathcal{R}$ are defined under the same syntax definition system. In practice, there are many well-developed syntax definition systems, each of which operate on subtly different classes or encodings of grammars, or that do not use grammars at all (we consider these in detail in Sec. \ref{sec:syntax-dialects}.) If the dialect designers  have not  picked the same syntax definition system, ``dialect combination'' remains a strictly informal notion.%$\mathcal{H} \cup \mathcal{R}$ is simply undefined.% (e.g. parser combinator libraries like Haskell's \li{parsec} \cite{parsec}.)

If $\mathcal{H}$ and $\mathcal{R}$ are coincidentally defined under the same system, we must also have that this system operationalizes the notion of dialect combination, i.e. we need some operation $\mathcal{H} \cup \mathcal{R}$ to be well-defined. Under systems that do not define such an operation (e.g. Racket's dialect preprocessor \cite{Flatt:2012:CLR:2063176.2063195}), clients can only manually  ``copy-and-paste'' or factor out portions of the constituent dialect definitions to construct the ``combined'' dialect. This is not systematic and, in practice, tedious and error-prone.%In both this and the previous case, ``dialect combination'' is a strictly informal notion, left to library clients to operationalize through manual labor (hence the quotes).

If we restrict our interest  to dialects specified using a single syntax definition system that does operationalize the notion of dialect combination (or equivalently one that allows clients to systematically combine \emph{dialect fragments}), there is still a problem: these systems do not guarantee that the combined dialect will conserve important properties that can be established about the constituent dialects in isolation (i.e. \emph{modularly}.) In other words, establishing $P(\mathcal{H})$ and $P(\mathcal{R})$ is not sufficient to establish $P(\mathcal{H} \cup \mathcal{R})$. Clients must re-establish such properties for each combined dialect that they construct.%In other words, any putative ``combined language'' must formally be considered a  distinct system for which one must derive essentially all metatheorems of interest anew, guided only informally by those derived for the dialects individually. %There is no well-defined mechanism for constructing such a ``combined language'' in general. 

For example, consider two syntax dialects defined using a system like Camlp4: $\mathcal{D}_1$ defines derived forms for sets, and $\mathcal{D}_2$ defines derived forms for finite maps, both delimited by \verb~{|~ and \verb~|}~.\footnote{In OCaml, simple curly braces are already reserved by the language for record types and values.} Though each dialect defines a deterministic grammar, i.e. $\mathrm{det}(\mathcal{D}_1)$ and $\mathrm{det}(\mathcal{D}_2)$, when the grammars are na\"ively combined by Camlp4, it is not the case that $\mathrm{det}(\mathcal{D}_1 \cup \mathcal{D}_2)$ (i.e. syntactic ambiguities arise under the combined dialect.) In particular, the empty set and the empty dictionary are both drawn \verb~{||}~. %A third syntax dialect might come along that uses the same forms that $\mathcal{D}_2$ defines, but for ordered finite maps.

Schwerdfeger and Van Wyk describe a modular analysis, implemented in Copper \cite{conf/gpce/WykS07}, that ``nearly'' guarantees that determinism is conserved when syntax dialects (of a certain restricted class) are combined \cite{conf/pldi/SchwerdfegerW09}, the caveat being that the constituent dialects must prefix all newly introduced forms with starting tokens drawn from disjoint sets. We will return to this requirement (and some other subtle requirements) in Sec. \ref{sec:syntax-dialects}.


\subsubsection{Abstract Reasoning About Derived Forms}\label{sec:abs-reasoning-intro}
Even putting aside the difficulties of conservatively combining syntax dialects, there are questions about how \emph{reasonable}  sprinkling library-specific derived forms throughout a large software system might be. 
For example, consider the perspective of a programmer attempting to comprehend (i.e. reason about) the following program fragment drawn under a syntax dialect constructed by combining SML's textual syntax with a large number of other syntax dialects:
\begin{lstlisting}
val a = get_a()
val w = get_w()
val x = read_data(a)
val y = {|(!R)@&{&/x!/:2_!x}'!R}|}
\end{lstlisting}

If the programmer happens to be familiar with the intentionally terse syntax of the stack-based database query processing language K, then this might pose few difficulties. If the programmer does not recognize this syntax, however, there is no simple, definitive protocol for answering questions like:

\begin{enumerate}
\item Which constituent dialect defined the derived form that appears on Line 4?
\item Is the character \li{x} inside this derived form parsed as a ``spliced'' expression, \li{x}, or parsed in some other way peculiar to this derived form?
\item If \li{x} is the spliced expression \li{x}, does it refer to the binding on the previous line? Or was that binding shadowed by an unseen binding in the desugaring of line 4?
\item If \li{w} is renamed, could that possibly break the program, or change its meaning? In other words, does the desugaring  assume that some variable identified as \li{w} is in scope (though \li{w} does not appear directly in the text of Line 4)?
\item What type does \li{y} have?
\end{enumerate}
In summary, syntax dialects do not maintain \emph{syntactic abstraction} -- if the desugaring of the program is held abstract, programmers can no longer reason about types and binding in the usual disciplined manner. This is burdensome at all scales, but particularly when programming in the large, where it is quite common to encounter a program fragment drawn by another programmer, or drawn by the programmer long ago.

%In other words, encountering an unfamiliar derived form has made it difficult for the programmer to maintain the usual \emph{type discipline} and \emph{binding discipline}. %Compelling the programmer to examine the desugaring directly defeat the purpose of defining the derived form -- decreasing cognitive cost. Indeed, it substantially increases cognitive cost.

In contrast, when a programmer encounters, for example, a function call like the call to \li{read_data} on Line 3, the analagous questions can be answered by following clear protocols that become ``cognitive reflexes'' after sufficient experience with the language, even if the programmer has no experience with the library defining \li{read_data}:
\begin{enumerate}
\item The language's syntax definition determines that \li{read_data(a)} is an expression of function application form.
\item \li{read_data} and \li{a} are expressions of variable form.
\item The variable \li{a} can only refer to the binding of \li{a} on Line 1.
\item The variable \li{w} can be renamed without knowing anything about the values that \li{read_data} and \li{a} stand for.
\item The type of \li{x} can be determined to be \li{B} by first determining that the type of \li{read_data} is \li{A -> B} for some \li{A} and \li{B}, and then checking that \li{a} has type \li{A}. Nothing else needs to be known about the values that \li{read_data} and \li{a} stand for. In the words of Reynolds \cite{B304}:
\begin{quote}
\emph{Type structure is a syntactic discipline for enforcing levels of abstraction.}
\end{quote}
\end{enumerate}


%A related issue arises when one works within a language with a module system, i.e. a system that supports interacting through a defined interface with various implementations of that interface. For example, consider different regular expression engines that differ only with regard to their performance in various circumstances, or different parser generators that accept the same class of grammar. Ideally, one would like to be able to define derived forms once such that they operate only through the common interface. To do so today requires both an awkward syntactic trick and coordination between library providers, as we will discuss in Sec. \ref{sec:syntax-examples-regexps}. Ideally, this would not be necessary.

%It is thus infeasible to simply allow different contributors to a software system to choose their own favorite dialect for each component they are responsible for. 
%It it clear that dialects are better rhetorical devices than practical engineering artifacts. 

%Due to this paucity of modular reasoning principles, the ``dialect-oriented'' approach is problematic for software development ``in the large''. %Large software projects and software ecosystems must pick a single language that does provide powerful modular reasoning principles and, to benefit from them, stay inside it.

% \subsection{Central Planning Considered Harmful}
% Dialects do sometimes have a less direct influence on large-scale software development: they can help convince the designers in control of comparatively popular languages, like OCaml and Scala, to include some variant of the primitives that they feature into backwards-compatible language revisions. %These decisions are increasingly influenced by community processes, e.g. the Scala Improvement Process.  %This approach concentrates power as well as responsibility over maintaining metatheoretic guarantees in the hands of a small group of language designers, though increasingly influenced by various community processes (e.g. the Scala Improvement Process). 
% %Dialects thus serve the role of rhetorical vehicles for new ideas, rather than direct artifacts. 
% %Over time, accepting such extensions has caused these languages to balloon in size. 
% This \emph{ad hoc} approach is unsustainable, for three main reasons. First, as we will demonstrate in Sec. \ref{sec:motivating-examples}, there are simply too  many potentially useful such primitives, and many of these capture idioms common only in relatively narrow application domains. It is unreasonable to expect language designers to be able to evaluate all of these use cases in a timely and informed manner. Second, primitives introduced earlier in a language's lifespan can end up monopolizing finite ``syntactic resources'', forcing subsequent primitives to use ever more esoteric forms. And third, primitives that prove after some time to be flawed in some way cannot be removed or modified without breaking backwards compatibility. For these reasons, language designers are justifiably reticent to add new primitives to major languages.%Because there is often no empirical data about how useful a construct is in practice until it is available in a major language, decisions about which constructs to include are often informed only by intuition (and are thus)
% %Recalling the words of  Reynolds, which are clearly as relevant today as they were almost half a century ago \cite{Reynolds70}: %This approach is antithetical to the ideal of a truly \emph{general-purpose language} described at the beginning of this section.
% %\newpage

%\subsection{Toward More Reasonable Primitives}
%These 
%This leaves two possible paths forward. One is to simply eschew ``niche'' derived forms and settle on the existing designs, which might be considered to sit at a ``sweet spot'' in the overall language design space (accepting that in some circumstances, this leads to  high cognitive cost). 


%Similarly, it recently introduced ``open datatypes'', which subsume its previous more specialized exception type, and captures many use cases for .

%Viewed ``dually'', one might equivalently ask for a language that builds in a core that is as small as possible, but provides expressive power comparable to languages with much larger cores. This is our goal in the work being proposed

%Similarly, it recently introduced ``open datatypes'', which subsume its previous more specialized exception type, and captures many use cases for .

%Viewed ``dually'', one might equivalently ask for a language that builds in a core that is as small as possible, but provides expressive power comparable to languages with much larger cores. This is our goal in the work being proposed. 

%\vspace{-10px}
\section{Contributions}\label{sec:contributions}
%%Our broad aim in the work being proposed is to introduce primitive language mechanisms that give library providers the ability to  express new syntactic expansions as well as new types and operators in a safe and modularly composable manner. 
To summarize our motivating argument: the widespread proliferation of syntax dialects and syntax definition systems suggests that programmers value library-specific (a.k.a. domain-specific) syntactic sugar. However, the dialect-oriented approach seems to be incompatible with the best practices of programming in the large.  

Our aim is to develop a more reasonable approach to the problem of defining library-specific syntactic sugar. This approach is oriented around a new language construct -- the \textbf{typed syntax macro (TSM)}. Client programmers apply TSMs to trees of \emph{generalized literal form}. Four examples where a TSM (identified by a \li{#\dolla#}-prefixed name) is applied (in post-fix position) to a tree of generalized literal form are given in Figure \ref{fig:first-tsm-example}.
\begin{figure}[h]
\begin{lstlisting}
[SURL1, 2, 3, 4, 5EURL] $intlist
`SURL<p>Hello, <$name></p>EURL` $html
/SURL\d\d\d-\d\d-\d\d\d\dEURL/ $rx
{|SURL(!R)@&{&/x!/:2_!x}'!R}EURL|} $K
\end{lstlisting}
\caption{Four trees of TSM application form, each of which consists of a tree of generalized literal form and a TSM name.}
\label{fig:first-tsm-example}
\end{figure}

Generalized literal forms subsume many common syntactic forms in that they are only definitively delimited by the context-free syntax of the language -- their \emph{bodies} (in blue above) are otherwise syntactically unconstrained. The context-free syntax cannot be extended by library providers, so syntactic conflicts between libraries are simply not a concern. Instead, the available generalized literal forms are contextually repurposed. In particular, the applied TSM is delegated control over the parsing and expansion of the literal body during a semantic phase known as \emph{typed expansion}, which generalizes the usual typing phase. As such, the semantics can take the type and binding structure of the surrounding program into account when validating the expansion that the TSM generates to ensure that clients are not forced to inspect the final expansion to answer questions like those enumerated in Section \ref{sec:abs-reasoning-intro}. In other words, TSMs come equipped with useful abstract reasoning principles. We will, of course, more precisely characterize these reasoning principles as we proceed.

% There is also no ambiguity with regard to which TSM has control over each form, and searching for the definition of a TSM is no more difficult than searching for any other binding, i.e. there are well-defined scoping rules.

% In other words, TSMs maintain a useful notion of syntactic abstraction. %More specifically, TSMs maintain a \emph{hygienic binding discipline}, meaning that questions Questions 4 and 5 above were concerned with are disallowed entirely. 
% We will, of course, make this notion more technically precise as we continue.

We will begin by integrating TSMs into a simple language of expressions and types in Chapter \ref{chap:uetsms}. We will then add support for  structural pattern matching  in Chapter \ref{chap:uptsms}. In Chapter \ref{chap:ptsms}, we add an ML-style module system and introduce \emph{parametric TSMs}, i.e. TSMs that take type and module parameters. Parameters serve two purposes: 1) they allow the expansions that TSMs generate to interact with the surrounding context in a controlled and reasonable manner; and 2) they enable TSMs that operate not just at a single type, but over a type- and module-parameterized family of types. Support for partial parameter application in TSM abbreviations decreases the syntactic cost of this explicit parameter passing style.

We make a simplifying assumption in these first chapters: that each TSM definition is self-contained, needing no access to libraries beyond the standard library. This allows us to focus on more fundamental issues, but it is, of course, an impractical assumption. We relax this assumption in Chapter \ref{chap:static-eval}, introducing the notion of a \emph{static environment} shared between TSM definitions.

%\item \textbf{Type-specific languages}, or \textbf{TSLs}. TSLs, described 
In Chapter \ref{chap:tsls}, we show how library clients can contextually designate, for any type, a privileged TSM at that type. We define a bidirectional typed expansion system that  invokes the designated TSM implicitly whenever an unadorned literal form appears and an expected type can be determined. This method of \emph{TSM implicits} can decrease the syntactic cost of many idioms to very nearly the same extent that library-specific dialects can, while still maintaining the reasoning principles characteristic of our approach.
%\item \textbf{Metamodules}, introduced in Sec. \ref{sec:metamodules}, reduce the need to primitively build in the type structure of constructs like records (and variants thereof),  labeled sums and other interesting constructs that we will introduce later by giving library providers programmatic ``hooks'' directly into the semantics, which are specified as a \emph{type-directed translation semantics} targeting a small \emph{typed internal language} (introduced in Sec. \ref{sec:VerseML}). %For example, a library provider can implement the type structure of records with a metamodule that:
%\begin{enumerate}
%\item introduces a type constructor, \lstinline{record}, parameterized by finite mappings from labels to types, and defines, programmatically, a translation to unary and binary product types (which are built in to the internal language); and 
%\item introduces operators used to work with records, minimally record introduction and elimination (but perhaps also various functional update operators), and directly implements the logic governing their typechecking and translation to the IL (which builds in only nullary and binary products). 
%\end{enumerate}
%We will see direct analogies between ML-style modules (which our mechanisms also support) and metamodules later.
%\end{enumerate} 


As vehicles for this work, we will define a small programming language in each of the chapters just mentioned, each building conceptually upon the previous one. All formal contributions of this work are relative to these small languages.

In examples, we assume a full-scale functional language called VerseML.\footnote{We distinguish VerseML from Wyvern, which is the language described in our prior publications about some of the work that we will describe, because Wyvern is a group effort evolving independently.} VerseML is the language of Chapter \ref{chap:tsls}  extended with some additional conveniences that are commonly found in other functional languages and, notionally, orthogonal to TSMs (e.g. higher-rank polymorphism \cite{conf/icfp/DunfieldK13}, signature abbreviations, and syntactic sugar that is not library-specific, e.g. for curried functions.) %VerseML is, as its name suggests, a conceptual descendent of ML. It diverges from other dialects of ML that have a similar type structure in that it has a bidirectional type system \cite{Pierce:2000:LTI:345099.345100} (like, for example, Scala \cite{OdeZenZen01}) for reasons that have to do with the mechanism of TSM implicits described in Chapters \ref{chap:tsls} and \ref{chap:ptsms}. 
%The reason we will not follow Standard ML \cite{mthm97-for-dart} in giving a complete formal definition of VerseML in this work is both to emphasize that the primitives we introduce are ``insensitive'' to the details of the underlying type structure of the language (so TSMs can be considered for inclusion in a variety of languages, not only dialects of ML), and to avoid distracting the reader (and the author) with definitions that are already well-understood in the literature and that are orthogonal to those that are the focus of this work. 
We will not formally define these features mainly to avoid unnecessarily complicating our presentation with details that are not essential to the ideas presented herein -- our purpose is only to introduce TSMs, not to give a complete definition of  a new full-scale programming language. As such, all examples involving VerseML should be understood to be informal motivating material for the subsequent formal material. %We anticipate that future full-scale language specifications will be able to combine the ideas  in the proposed work without trouble. %The purpose of the work being proposed is to serve as a reference for those interested in the new constructs we introduce, not to serve as a language specification. 
%We will give a brief overview of these languages are organized in Sec. \ref{sec:VerseML}.

%TSMs, like other macro systems, perform \emph{static code generation} (also sometimes called \emph{static} or \emph{compile-time metaprogramming}), meaning that the relevant rules in the static semantics of the language call for the evaluation of \emph{static functions} that generate term encodings. Static functions are functions that are evaluated statically, i.e. during typing. %Library providers write these static functions using the VerseML \emph{static language} (SL).  
%Maintaining a separation between the static (or ``compile-time'') phase and the dynamic (or ``run-time'') phase is an important facet of VerseML's design. % static code generation. %We will  also introduce a simple variant of each of these primitives that leverages VerseML's support for local type inference to further reduce syntactic cost in certain common situations. 


\subsection*{Thesis Statement}
In summary, this work defends the following statement:

\begin{quote}
A functional programming language can give library providers the ability to %meta\-pro\-gram\-matic\-ally 
programmatically control the parsing and expansion of expressions and patterns of generalized literal form while maintaining a reasonable type and binding discipline. %These  primitives are  expressive enough to subsume the need for a variety of primitives that are, or would need to be, built in to comparable contemporary languages.
\end{quote}
\section{Disclaimers}
Before we continue, it may be prudent to explicitly acknowledge that eliminating the need for dialects would indeed be asking for too much: certain syntax design decisions are fundamentally incompatible with others or require coordination across a language design. We aim only to diminish the need for syntax dialects, not to give control over all design decisions to library providers. %We summarize some of the situations that we explicitly do not consider here in Sec. \ref{sec:future-work}. % out a larger design space within a single language, VerseML.%a subset of constructs that can be specified by a semantics of a certain ``shape'' specified by VerseML (we will make this more specific later). %There is nothing ``universal'' about VerseML.

It may also be prudent to explicitly acknowledge that library providers could use TSMs  to define syntactic forms that are ``in poor taste.'' In practice, a language with support for TSMs should come with a standard library defining an expertly curated collection of TSMs, as well as guidelines for advanced users on when it would be prudent to define their own TSMs (following the example of languages that support operator overloading or \emph{ad hoc} polymorphism using type classes \cite{Hall:1996:TCH:227699.227700,conf/popl/DreyerHCK07}, which also have some potential for ``abuse'' or ``overuse''.) %For most programmers, using VerseML will not require explicitly defining a TSM on their own.%be substantially different from using a language like ML or one of its dialects. 
The majority of programmers should very rarely need to define a TSM on their own.

%Finally, VerseML is not designed as a dependently-typed language like Coq, Agda or Idris. %because these languages do not maintain a phase separation between ``compile-time'' and ``run-time.'' This phase separation is useful for programming tasks (where one would like to be able to discover errors before running a program, particularly programs that may have an effect) but less so for theorem proving tasks (where it is mainly the fact that a pure expression is well-typed that is of interest, by the propositions-as-types principle). 
